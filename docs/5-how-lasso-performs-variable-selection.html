<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 5 How Lasso performs variable selection | Shrinkage Methods" />
<meta property="og:type" content="book" />






<meta name="date" content="2020-04-24" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Chapter 5 How Lasso performs variable selection | Shrinkage Methods">

<title>Chapter 5 How Lasso performs variable selection | Shrinkage Methods</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="has-sub"><a href="index.html#motivation"><span class="toc-section-number">1</span> Motivation</a><ul>
<li><a href="1-1-the-ordinary-least-squared-model.html#the-ordinary-least-squared-model"><span class="toc-section-number">1.1</span> The ordinary least squared model</a></li>
<li><a href="1-2-several-solutions-proposed-by-statisticians.html#several-solutions-proposed-by-statisticians"><span class="toc-section-number">1.2</span> Several solutions proposed by statisticians</a></li>
<li><a href="1-3-shrinkage.html#shrinkage"><span class="toc-section-number">1.3</span> Shrinkage</a></li>
</ul></li>
<li class="has-sub"><a href="2-ridge-regression.html#ridge-regression"><span class="toc-section-number">2</span> Ridge Regression</a><ul>
<li><a href="2-1-definition.html#definition"><span class="toc-section-number">2.1</span> Definition</a></li>
<li><a href="2-2-ridge-regression-1.html#ridge-regression-1"><span class="toc-section-number">2.2</span> Ridge Regression:</a></li>
</ul></li>
<li class="has-sub"><a href="3-how-ridge-works.html#how-ridge-works"><span class="toc-section-number">3</span> How Ridge Works</a><ul>
<li><a href="3-how-ridge-works.html#univariate-example"><span class="toc-section-number">3.0.1</span> Univariate example</a></li>
<li><a href="3-how-ridge-works.html#higher-dimension-ridge-regression"><span class="toc-section-number">3.0.2</span> Higher dimension ridge regression</a></li>
<li><a href="3-how-ridge-works.html#the-tuning-parameter"><span class="toc-section-number">3.0.3</span> The tuning parameter</a></li>
</ul></li>
<li class="has-sub"><a href="4-lasso.html#lasso"><span class="toc-section-number">4</span> Lasso</a><ul>
<li><a href="4-1-standardize-the-dataset.html#standardize-the-dataset"><span class="toc-section-number">4.1</span> Standardize the dataset</a></li>
</ul></li>
<li class="has-sub"><a href="5-how-lasso-performs-variable-selection.html#how-lasso-performs-variable-selection"><span class="toc-section-number">5</span> How Lasso performs variable selection</a><ul>
<li><a href="5-how-lasso-performs-variable-selection.html#univariate-example-1"><span class="toc-section-number">5.0.1</span> univariate example</a></li>
<li><a href="5-how-lasso-performs-variable-selection.html#shrinkage-explaination-from-geometric-perspective"><span class="toc-section-number">5.0.2</span> Shrinkage explaination from geometric perspective</a></li>
</ul></li>
<li class="has-sub"><a href="6-a-real-example.html#a-real-example"><span class="toc-section-number">6</span> A Real Example</a><ul>
<li><a href="6-a-real-example.html#cross-validation"><span class="toc-section-number">6.0.1</span> Cross validation</a></li>
<li><a href="6-a-real-example.html#ols"><span class="toc-section-number">6.0.2</span> OLS</a></li>
<li><a href="6-a-real-example.html#ridge"><span class="toc-section-number">6.0.3</span> RIDGE</a></li>
<li><a href="6-a-real-example.html#lasso-1"><span class="toc-section-number">6.0.4</span> LASSO</a></li>
<li><a href="6-a-real-example.html#test-data"><span class="toc-section-number">6.0.5</span> Test data</a></li>
<li><a href="6-a-real-example.html#activity"><span class="toc-section-number">6.0.6</span> Activity</a></li>
<li><a href="6-a-real-example.html#ols-1"><span class="toc-section-number">6.0.7</span> OLS</a></li>
<li><a href="6-a-real-example.html#ridge-1"><span class="toc-section-number">6.0.8</span> RIDGE</a></li>
<li><a href="6-a-real-example.html#lasso-2"><span class="toc-section-number">6.0.9</span> LASSO</a></li>
<li><a href="6-a-real-example.html#test-data-1"><span class="toc-section-number">6.0.10</span> Test data</a></li>
<li><a href="6-1-data-summary.html#data-summary"><span class="toc-section-number">6.1</span> Data Summary</a></li>
</ul></li>
<li class="has-sub"><a href="7-bayesian-lasso.html#bayesian-lasso"><span class="toc-section-number">7</span> Bayesian Lasso</a><ul>
<li><a href="7-1-generalization-of-shrinkage-methods-from-bayes-perspective.html#generalization-of-shrinkage-methods-from-bayes-perspective"><span class="toc-section-number">7.1</span> Generalization of shrinkage methods from Bayes perspective</a></li>
<li class="has-sub"><a href="7-2-bayesian-interpretation.html#bayesian-interpretation"><span class="toc-section-number">7.2</span> Bayesian Interpretation</a><ul>
<li><a href="7-2-bayesian-interpretation.html#posterior"><span class="toc-section-number">7.2.1</span> Posterior</a></li>
<li><a href="7-2-bayesian-interpretation.html#posterior-mode"><span class="toc-section-number">7.2.2</span> Posterior Mode</a></li>
</ul></li>
</ul></li>
<li><a href="8-limitations-of-ridge-and-lasso.html#limitations-of-ridge-and-lasso"><span class="toc-section-number">8</span> Limitations of Ridge and Lasso</a></li>
<li><a href="9-reference.html#reference"><span class="toc-section-number">9</span> Reference</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="how-lasso-performs-variable-selection" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> How Lasso performs variable selection</h1>
<p>Again, just as we discussed how shrinkage worked, we’ll use the univariate model to illustrate.</p>
<div id="univariate-example-1" class="section level3">
<h3><span class="header-section-number">5.0.1</span> univariate example</h3>
<p>Again we have a simple model: <span class="math inline">\(y = \beta x + \epsilon\)</span>, with an L2 penalty(Lasso regression) on <span class="math inline">\(\hat\beta\)</span> and a least squares loss function on <span class="math inline">\(\hat\epsilon\)</span>. We can then expand the expression for sum of squared residuals to be minimized as:
<span class="math display">\[
\hat\beta = arg\min_\beta\{\sum_{i = 1}^{N}(y_i - x_i\beta)^2 + \lambda\sum_{j = 1}^{p}\rvert\beta_j\rvert\}
\]</span>
For convenience, we will now substitute <span class="math inline">\(2\lambda^* = \lambda\)</span>, since <span class="math inline">\(2\lambda^* = \lambda\)</span> has a one-to-one relationship, this change does not affect the result. We can then rewrite <span class="math inline">\(\hat\epsilon\)</span> in the matrix form:
<span class="math display">\[
\hat{\beta} = arg\min_\beta\{(\vec{y} - \vec{x}\hat\beta)^{T}(\vec{y} - \vec{x}\hat\beta) + 2\lambda^*\rvert\hat{\beta}\rvert\}
\]</span>
Which we can further expand into:
<span class="math display">\[
\hat\beta = arg\min_\beta\{\vec{y}^{T}\vec{y} - 2\vec{y}^T\vec{x}\hat{\beta} + \hat{\beta}\vec{x}^T\vec{x}\hat{\beta} + 2\lambda^*\rvert\hat{\beta}\rvert\}
\]</span>
Unlike the ridge regression case where we can take a derivative directly and set it equal to zero so that we can minimize the residual, here we have a <span class="math inline">\(\rvert\hat{\beta}\rvert\)</span> term which makes such procedure painful. Thus, we will instead compare different cases w.r.t. <span class="math inline">\(\hat\beta\)</span>.
Case1: <span class="math inline">\(\hat\beta \geq 0\)</span>, <span class="math inline">\(\rvert\hat{\beta}\rvert = \hat{\beta}\)</span>
Since we are assuming that <span class="math inline">\(\hat\beta \geq 0\)</span>, it is the same as assuming <span class="math inline">\(\vec{y}^T\vec{x} \geq 0\)</span>(x’s and y’s have a positive relationship).
In this case, we can rewrite the above expression as:
<span class="math display">\[
\hat\epsilon = arg\min_\beta\{\vec{y}^{T}\vec{y} - 2\vec{y}^T\vec{x}\hat{\beta} + \hat{\beta}\vec{x}^T\vec{x}\hat{\beta} + 2\lambda^*\hat{\beta}\}
\]</span>
Then we can take the derivative w.r.t. <span class="math inline">\(\hat\beta\)</span> and set it equal to zero:
<span class="math display">\[
-2\vec{y}^T\vec{x} + 2\vec{x}^T\vec{x}\hat{\beta} + 2\lambda^* =_{set} 0
\]</span>
Where we can obtain a solution for <span class="math inline">\(\hat\beta\)</span>:
<span class="math display">\[
\hat\beta = (\vec{y}^T\vec{x} - \lambda^*)(\vec{x}^T\vec{x})^{-1}
\]</span>
Obviously by increasing <span class="math inline">\(\lambda^*\)</span>, we can eventually achieve <span class="math inline">\(\hat\beta = 0\)</span> at <span class="math inline">\(\lambda^* = \vec{y}^T\vec{x}\)</span>. However, it is tricky to think about what happens when we increase <span class="math inline">\(\lambda^*\)</span> once <span class="math inline">\(\vec{y}^T\vec{x} = 0\)</span>. The thing is that increasing <span class="math inline">\(\lambda^*\)</span> at this point will not drive <span class="math inline">\(\hat{\beta}\)</span> negative, because once the estimator <span class="math inline">\(\hat\beta\)</span> becomes negative, the derivative of the penalty term estimator becomes:
<span class="math display">\[
-2\vec{y}^T\vec{x} + 2\vec{x}^T\vec{x}\hat{\beta} - 2\lambda^* =_{set} 0
\]</span>
where the flip in the sign of <span class="math inline">\(\lambda^*\)</span> is due to the absolute value function before taking the derivative. Thus, we have a new solution for <span class="math inline">\(\hat\beta\)</span>:
<span class="math display">\[
\hat\beta = (\vec{y}^T\vec{x} + \lambda^*)(\vec{x}^T\vec{x})^{-1}
\]</span>
This solution, however, is inconsistent with our premises <span class="math inline">\(\hat\beta &lt; 0\)</span>, since we have assumed that the least squares solution is greater than or equal to zero(<span class="math inline">\(\vec{y}^T\vec{x} \geq 0\)</span>), and <span class="math inline">\(\lambda^* \geq 0\)</span>. For this solution, the sum of squared residual does not have an minimum anymore. Thus, we will just stick at <span class="math inline">\(\hat\beta = 0\)</span>, even if <span class="math inline">\(\lambda^* &gt; \vec{y}^T\vec{x}\)</span>.
Intuitively, when we assume that the least squares solution is negative with <span class="math inline">\(\hat\beta &lt; 0\)</span>, the logic is the same that we stick with <span class="math inline">\(\hat\beta = 0\)</span> once it reaches zero.
Note that so far we’ve only talked about a univariate Lasso example. When having a dataset that has multiple dimensions, as we keep increasing the value of <span class="math inline">\(\lambda\)</span>(or <span class="math inline">\(\lambda^*\)</span>), some of the features or variables will be zeroed out just as <span class="math inline">\(\hat\beta\)</span> shown above while some other features are shrinked toward zero but not yet reduced to zero. Therefore, the Lasso does variable selection in this manner, by shrinking some of the coefficients to zero while some non-zero.</p>
</div>
<div id="shrinkage-explaination-from-geometric-perspective" class="section level3">
<h3><span class="header-section-number">5.0.2</span> Shrinkage explaination from geometric perspective</h3>
<p>Now suppose that we have a dataset with 2 features. We first train a liner model for the data, and suppose <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are the coefficients for the two features. Thus by the definition of the ridge regression constraint, we have:
<span class="math display">\[
\beta_1^2 + \beta_2^2 \leq t
\]</span>
Similarly, for the Lasso regression contraint, we have:
<span class="math display">\[
\rvert\beta_1\rvert + \rvert\beta_2\rvert \leq t
\]</span>
Thus, if we plot the two constraints in a 2-dimensional coordinate system, we have:
<img src="2dimConstraint.png" /></p>
<p>Note that the red contours are the vector space for the coefficient estimator <span class="math inline">\(\hat\beta\)</span>. Originally, the constraint and the vector space are separate; as we increase the constraint limit <span class="math inline">\(t\)</span>, eventually the two constraints will hit the vector space, and thus achieving optimization for both the ridge regression model and the Lasso regression model. Since the 2-dimensional Lasso constraint has corners(diamond shape), if the constraint hit the vector space on one such corner, one of the two features gets dropped and shrinked to zero. In higher dimensional spaces, the diamond shape becomes rhomboid where there are more corners and more possibility for dropping one or more variables. In contrast, the constraint for ridge regression is a circle, where all points on or within the circle resemble a linear combination of the two features. Thus, technically speaking, when the ridge regression constraint hit the vector space and achieve optimization, one variable could be shrunk very close to zero while the other remain slightly shrunk, it is impossible for ridge regression to achieve variable selection in reality. Similarly, in higher dimensional space, the constraint for ridge regression becomes a shpere and the situation is very similar to what we have in the two dimensional space.</p>

</div>
</div>
<p style="text-align: center;">
<a href="4-1-standardize-the-dataset.html"><button class="btn btn-default">Previous</button></a>
<a href="6-a-real-example.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
